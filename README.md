# DeepNLPProjects

Deep NLP projects containing:

1. Toxic comments multi- label classification using CNN and RNNs( LSTM and GRU).
2. Sequence to Sequence Learning with Neural Networks (https://github.com/bentrevett/pytorch-seq2seq)
3. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation (https://github.com/bentrevett/pytorch-seq2seq)
4. Neural Machine Translation by Jointly Learning to Align and Translate (https://github.com/bentrevett/pytorch-seq2seq)
5. Packed Padded Sequences, Masking, Inference and BLEU (https://github.com/bentrevett/pytorch-seq2seq)
6. Convolutional Sequence to Sequence Learning (https://github.com/bentrevett/pytorch-seq2seq)
7. Attention Is All You Need (https://github.com/bentrevett/pytorch-seq2seq)
8. Visual Question Answering 


## References:

- https://www.coursera.org/learn/language-processing.  [ week 4,5]
- https://www.coursera.org/learn/attention-models-in-nlp [ all - seq, attention, transformers]
- https://github.com/bentrevett/pytorch-seq2seq



## Blogs and Research

- https://ruder.io/deep-learning-nlp-best-practices/index.html
- https://ruder.io/deep-learning-optimization-2017/index.html
- https://ruder.io/word-embeddings-2017/index.html
- https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/
- https://awni.github.io/train-sequence-models/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=The%20Wild%20Week%20in%20AI
- https://tryolabs.com/blog/2017/12/12/deep-learning-for-nlp-advancements-and-trends-in-2017/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=The%20Wild%20Week%20in%20AI
- http://jalammar.github.io/illustrated-transformer/
- http://jalammar.github.io/illustrated-bert/
- https://jalammar.github.io/illustrated-gpt2/?utm_campaign=NLP%20News&utm_medium=email&utm_source=Revue%20newsletter

## Flow

word2vec --> seq2seq with attention --> transformer --> BERT, ELMO --> GPT2/GPT3

- http://jalammar.github.io/

- https://towardsdatascience.com/attention-and-its-different-forms-7fc3674d14dc  (Types of attention)


## cs224n-Natural-Language-Processing-with-Deep-Learning
Course under stanford: Natural Language Processing with Deep Learning

Link: http://web.stanford.edu/class/cs224n/

Topics:

- Introduction and Word Vectors
- Gensim word vectors example
- Word Vectors 2 and Word Senses
- Word Window Classification, Neural Networks, and PyTorch
- Matrix Calculus and Backpropagation
- Linguistic Structure: Dependency Parsing
- The probability of a sentence? Recurrent Neural Networks and Language Models
- Vanishing Gradients and Fancy RNNs
- Machine Translation, Seq2Seq and Attention
- Practical Tips for Final Projects
- Question Answering, the Default Final Project, and an introduction to Transformer architectures
- ConvNets for NLP
- Information from parts of words (Subword Models)
- Contextual Word Representations: BERT 
- Modeling contexts of use: Contextual Representations and Pretraining. ELMo and BERT.
- Natural Language Generation
- Reference in Language and Coreference Resolution
- Fairness and Inclusion in AI
- Constituency Parsing and Tree Recursive Neural Networks
- HuggingFace Transformers
- Recent Advances in Low Resource Machine Translation
- Analysis and Interpretability of Neural NLP
- GPT3 language model (https://www.youtube.com/watch?v=SY5PvZrJhLE)


Youtube:

https://www.youtube.com/watch?v=8rXD5-xhemo&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z


#### Work to be done: - coursera nlp deeplearning.ai 

- Attention models
- Transformers(BERT, GPT3)
- seq2seq models (Machine Translation)
